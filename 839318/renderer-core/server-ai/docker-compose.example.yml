version: '3.8'

services:
  server-ai:
    build: .
    container_name: server-ai
    ports:
      - "3001:3001"
    environment:
      - NODE_ENV=production
      - PORT=3001
      - HOST=0.0.0.0
      - USE_LLM=${USE_LLM:-false}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4o-mini}
      - RATE_LIMIT_ENABLED=${RATE_LIMIT_ENABLED:-true}
      - RATE_LIMIT_RPM=${RATE_LIMIT_RPM:-10}
      - TELEMETRY_DUMP_FILE=${TELEMETRY_DUMP_FILE:-false}
      - TELEMETRY_FILE_PATH=/tmp/ai-telemetry/telemetry.log
    volumes:
      # Persist telemetry logs
      - ./logs:/tmp/ai-telemetry
      - ./requests:/tmp/ai-requests
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "node", "-e", "require('http').get('http://localhost:3001/health', (r) => { process.exit(r.statusCode === 200 ? 0 : 1) })"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 5s
    depends_on:
      - redis
    networks:
      - server-network

  # Optional: Redis for rate limiter persistence (if needed in future)
  redis:
    image: redis:7-alpine
    container_name: server-ai-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped
    command: redis-server --appendonly yes
    networks:
      - server-network
    # Redis is optional - rate limiter works in-memory without it
    profiles:
      - with-redis

networks:
  server-network:
    driver: bridge

volumes:
  redis-data:

